{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "import sys\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import importlib\n",
    "import smlmodule\n",
    "\n",
    "from itertools import combinations\n",
    "from pprint import pprint\n",
    "\n",
    "\"\"\"\n",
    "https://bmjopen.bmj.com/content/10/9/e039338\n",
    "\n",
    "We have computed the number of COVID-19 infected people for each province and the infection \n",
    "rate based on the number of inhabitants from February 24th to March 13th (the date when the \n",
    "lockdown was decided), as reported by the official government website, updated with daily \n",
    "frequency.34 The number of PM exceedances were computed between February 9th and February 29th, \n",
    "as we had to take into account the maximum lag period of 14 days, which is the average time \n",
    "elapsed between the contagion and the first weeks of the Italian epidemic \n",
    "(February 24th to March 13th). \n",
    "\n",
    "period1 = ['2020-02-09', '2020-02-28'] # YEAR-MONTH-DAY --->>> CASI COVID ['2020-02-24', '2020-03-13']\n",
    "\n",
    "period2 = ['2020-02-09', '2020-03-06] # YEAR-MONTH-DAY --->>> CASI COVID ['2020-02-09', '2020-03-21']\n",
    "period3 = ['2020-08-29', '2020-09-01'] # YEAR-MONTH-DAY --->>> CASI COVID ['2020-09-12', '2020-10-15']\n",
    "period4 = ['2020-08-29', '2020-10-30'] # YEAR-MONTH-DAY --->>> CASI COVID ['2020-09-12', '2020-11-14']\n",
    "period5 = ['2020-05-15', '2020-08-15'] # YEAR-MONTH-DAY --->>> CASI COVID ['2020-06-01', '2020-09-01']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LIMIT = 0.80\n",
    "\n",
    "verbose = False\n",
    "paperpath = \"./data/particulate.csv\"\n",
    "agefeatspath = \"./data/provinceages.csv\"\n",
    "deprividxpath = \"./data/ID11_prov21.xlsx\"\n",
    "tabellecodicipath = \"./data/TabelleCodici.xlsx\"\n",
    "copernicopath = \"./data/name_region_province_statistics_2020.csv\"\n",
    "\n",
    "__provmaps__ = {\n",
    "    \"bolzano_bozen\": \"bolzano\",\n",
    "    \"bolzanobozen\": \"bolzano\",\n",
    "    \"vibovalentia\": \"vibo_valentia\",\n",
    "    \"laquila\": \"l_aquila\",\n",
    "    \"laspezia\": \"la_spezia\",\n",
    "    \"barlettaandriatrani\": \"bat\",\n",
    "    \"ascolipiceno\": \"ascoli_piceno\",\n",
    "    \"carboniaiglesias\": \"carbonia\",\n",
    "    \"reggioemilia\": \"reggio_nell_emilia\",\n",
    "    \"pesarourbino\": \"pesaro\",\n",
    "    \"monzabrianza\": \"monza\",\n",
    "    \"reggiocalabria\": \"reggio_di_calabria\",\n",
    "    \"forlicesena\": \"forli\",\n",
    "    \"massacarrara\": \"massa\",\n",
    "    \"verbanocusioossola\": \"verbania\",\n",
    "    \"verbano_cusio_ossola\": \"verbania\",\n",
    "    \"massa_carrara\": \"massa\",\n",
    "    \"monza_e_della_brianza\": \"monza\",\n",
    "    \"pesaro_e_urbino\": \"pesaro\",\n",
    "    \"forli__cesena\": \"forli\",\n",
    "    \"bolzano_/_bozen\": \"bolzano\",\n",
    "    \"barletta_andria_trani\": \"bat\",\n",
    "    \"sud_sardegna\": \"carbonia\",\n",
    "    \"forl√¨_cesena\": \"forli\"\n",
    "}\n",
    "\n",
    "pollutantsnames = \"avg_wco_period1_2020,\"+\\\n",
    "        \"avg_wnh3_period1_2020,\"+\\\n",
    "        \"avg_wnmvoc_period1_2020,\"+\\\n",
    "        \"avg_wno2_period1_2020,\"+\\\n",
    "        \"avg_wno_period1_2020,\"+\\\n",
    "        \"avg_wo3_period1_2020,\"+\\\n",
    "        \"avg_wpans_period1_2020,\"+\\\n",
    "        \"avg_wpm10_period1_2020,\"+\\\n",
    "        \"avg_wpm2p5_period1_2020,\"+\\\n",
    "        \"avg_wso2_period1_2020,\" +\\\n",
    "        \"sum_wnh3_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wnmvoc_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wno2_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wno_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wpans_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wpm10_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wpm2p5_ex_q75_period1_2020,\" +\\\n",
    "        \"sum_wo3_ex_q75_period1_2020,\" + \\\n",
    "        \"sum_wco_ex_q75_period1_2020,\" + \\\n",
    "        \"sum_wso2_ex_q75_period1_2020\"\n",
    "\n",
    "featurestobeused = \"density,\" + \\\n",
    "        \"commutersdensity,\" + \\\n",
    "        \"depriv,\" + \\\n",
    "        \"lat,\" + \\\n",
    "        \"Ratio0200ver65,\" + \\\n",
    "        \"avg_wpm10_period1_2020,\"+\\\n",
    "        \"avg_wpm2p5_period1_2020,\"+\\\n",
    "        \"avg_wco_period1_2020,\"+\\\n",
    "        \"avg_wnh3_period1_2020,\"+\\\n",
    "        \"avg_wnmvoc_period1_2020,\"+\\\n",
    "        \"avg_wno2_period1_2020,\"+\\\n",
    "        \"avg_wno_period1_2020,\"+\\\n",
    "        \"avg_wo3_period1_2020,\"+\\\n",
    "        \"avg_wpans_period1_2020,\"+\\\n",
    "        \"avg_wso2_period1_2020\"\n",
    "\n",
    "def filterprovname (inprov):\n",
    "    low = inprov.lower()\n",
    "    low = low.rstrip()\n",
    "    low = low.lstrip()\n",
    "    low = low.replace(\" \", \"_\")\n",
    "    low = low.replace(\"'\", \"_\")\n",
    "    low = low.replace(\"-\", \"_\")\n",
    "\n",
    "    return low\n",
    "\n",
    "def normalize_provname (indata, provcolumn, verbose):\n",
    "\n",
    "    dict_data = {}  \n",
    "    for c in indata.columns:\n",
    "        if verbose:\n",
    "            print(\"  \", c)\n",
    "        if c != provcolumn:\n",
    "            dict_data[c] = []\n",
    "    dict_data[\"prov\"] = []\n",
    "\n",
    "    for i, row in indata.iterrows():\n",
    "        for c in indata.columns:    \n",
    "            if c != provcolumn:\n",
    "                dict_data[c].append(row[c])\n",
    "            else:\n",
    "                low = filterprovname(row[c])\n",
    "                if low in __provmaps__:\n",
    "                    low = __provmaps__[low]\n",
    "\n",
    "                dict_data[\"prov\"].append(low)\n",
    "\n",
    "    #for v in dict_data:\n",
    "    #    print(v, \" \", len(dict_data[v]))\n",
    "\n",
    "    data = pd.DataFrame.from_dict(dict_data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper data \n",
      "Age features \n",
      "Copernico data \n",
      "DrepivIdx name \n",
      "Province list: \n",
      "   1   trapani\n",
      "   2   parma\n",
      "   3   como\n",
      "   4   asti\n",
      "   5   savona\n",
      "   6   agrigento\n",
      "   7   trento\n",
      "   8   cagliari\n",
      "   9   torino\n",
      "   10   siracusa\n",
      "   11   potenza\n",
      "   12   cuneo\n",
      "   13   rieti\n",
      "   14   napoli\n",
      "   15   campobasso\n",
      "   16   caserta\n",
      "   17   isernia\n",
      "   18   genova\n",
      "   19   lecce\n",
      "   20   pesaro\n",
      "   21   lecco\n",
      "   22   nuoro\n",
      "   23   cremona\n",
      "   24   frosinone\n",
      "   25   firenze\n",
      "   26   forli\n",
      "   27   latina\n",
      "   28   bergamo\n",
      "   29   roma\n",
      "   30   vibo_valentia\n",
      "   31   catania\n",
      "   32   foggia\n",
      "   33   teramo\n",
      "   34   biella\n",
      "   35   bolzano\n",
      "   36   milano\n",
      "   37   monza\n",
      "   38   macerata\n",
      "   39   benevento\n",
      "   40   padova\n",
      "   41   crotone\n",
      "   42   belluno\n",
      "   43   reggio_di_calabria\n",
      "   44   pescara\n",
      "   45   caltanissetta\n",
      "   46   lodi\n",
      "   47   cosenza\n",
      "   48   rimini\n",
      "   49   siena\n",
      "   50   carbonia\n",
      "   51   brescia\n",
      "   52   terni\n",
      "   53   vercelli\n",
      "   54   lucca\n",
      "   55   catanzaro\n",
      "   56   pistoia\n",
      "   57   vicenza\n",
      "   58   novara\n",
      "   59   perugia\n",
      "   60   bari\n",
      "   61   pavia\n",
      "   62   chieti\n",
      "   63   salerno\n",
      "   64   verona\n",
      "   65   fermo\n",
      "   66   sondrio\n",
      "   67   matera\n",
      "   68   viterbo\n",
      "   69   brindisi\n",
      "   70   reggio_nell_emilia\n",
      "   71   mantova\n",
      "   72   pisa\n",
      "   73   venezia\n",
      "   74   enna\n",
      "   75   oristano\n",
      "   76   livorno\n",
      "   77   la_spezia\n",
      "   78   ferrara\n",
      "   79   piacenza\n",
      "   80   varese\n",
      "   81   pordenone\n",
      "   82   udine\n",
      "   83   prato\n",
      "   84   palermo\n",
      "   85   sassari\n",
      "   86   taranto\n",
      "   87   arezzo\n",
      "   88   bat\n",
      "   89   treviso\n",
      "   90   alessandria\n",
      "   91   modena\n",
      "   92   ascoli_piceno\n",
      "   93   avellino\n",
      "   94   verbania\n",
      "   95   ragusa\n",
      "   96   massa\n",
      "   97   rovigo\n",
      "   98   l_aquila\n",
      "   99   gorizia\n",
      "   100   bologna\n",
      "   101   ancona\n",
      "   102   trieste\n",
      "   103   ravenna\n",
      "   104   imperia\n",
      "   105   messina\n",
      "   106   grosseto\n"
     ]
    }
   ],
   "source": [
    "tc = pd.ExcelFile(tabellecodicipath)\n",
    "\n",
    "idtoprov = {}\n",
    "province = tc.parse(\"Codice Provincia\")\n",
    "for val in province[[\"Codice Provincia\",\"Nome Provincia\"]].values:\n",
    "    if type(val[1]) != float:\n",
    "        idtoprov[int(val[0])] = val[1]\n",
    "        #print(int(val[0]), val[1])\n",
    "\n",
    "in_datapaper = pd.read_csv(paperpath, sep=\";\")\n",
    "in_deprividx =  pd.ExcelFile(deprividxpath).parse(\"Foglio1\")\n",
    "in_agefeatures = pd.read_csv(agefeatspath)\n",
    "in_agefeatures = in_agefeatures[in_agefeatures.Population2020 != 0.0]\n",
    "in_copernico = pd.read_csv(copernicopath)\n",
    "\n",
    "print(\"Paper data \")\n",
    "datapaper = normalize_provname(in_datapaper, \"Province\", False)\n",
    "print(\"Age features \")\n",
    "agefeatures = normalize_provname(in_agefeatures, \"Provincia\", False)\n",
    "print(\"Copernico data \") \n",
    "copernico = normalize_provname(in_copernico, \"nome_ita\", False)\n",
    "\n",
    "dict_deprividx = {}\n",
    "print(\"DrepivIdx name \")\n",
    "for c in in_deprividx.columns:\n",
    "    if verbose:\n",
    "        print(\"   \", c)   \n",
    "    dict_deprividx[c] = []\n",
    "dict_deprividx[\"prov\"] = []\n",
    "\n",
    "for i, row in in_deprividx.iterrows():\n",
    "    id = row[\"prov21\"]\n",
    "    prov = filterprovname(idtoprov[id])\n",
    "    \n",
    "    if prov in __provmaps__:\n",
    "        prov = __provmaps__[prov]\n",
    "    \n",
    "    #print(id, prov)\n",
    "    dict_deprividx[\"prov\"].append(prov)\n",
    "    for c in in_deprividx.columns:\n",
    "        dict_deprividx[c].append(row[c])\n",
    "\n",
    "\n",
    "deprividx = pd.DataFrame.from_dict(dict_deprividx)       \n",
    "\n",
    "provincelist = list(set(list(datapaper[\"prov\"].values)) & \\\n",
    "        set(list(deprividx[\"prov\"].values)) & \\\n",
    "        set(list(agefeatures[\"prov\"].values)) &\n",
    "        set(list(copernico[\"prov\"].values)))\n",
    "\n",
    "print(\"Province list: \")\n",
    "for i, p in enumerate(provincelist):\n",
    "    print(\"  \", i+1, \" \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 trapani\n",
      "2 parma\n",
      "3 como\n",
      "4 asti\n",
      "5 savona\n",
      "6 agrigento\n",
      "7 trento\n",
      "8 cagliari\n",
      "9 torino\n",
      "10 siracusa\n",
      "11 potenza\n",
      "12 cuneo\n",
      "13 rieti\n",
      "14 napoli\n",
      "15 campobasso\n",
      "16 caserta\n",
      "17 genova\n",
      "18 lecce\n",
      "19 pesaro\n",
      "20 lecco\n",
      "21 nuoro\n",
      "22 cremona\n",
      "23 frosinone\n",
      "24 firenze\n",
      "25 forli\n",
      "26 latina\n",
      "27 bergamo\n",
      "28 roma\n",
      "29 vibo_valentia\n",
      "30 catania\n",
      "31 foggia\n",
      "32 teramo\n",
      "33 biella\n",
      "34 bolzano\n",
      "35 milano\n",
      "36 monza\n",
      "37 macerata\n",
      "38 benevento\n",
      "39 padova\n",
      "40 crotone\n",
      "41 belluno\n",
      "42 reggio_di_calabria\n",
      "43 pescara\n",
      "44 caltanissetta\n",
      "45 lodi\n",
      "46 cosenza\n",
      "47 rimini\n",
      "48 siena\n",
      "49 brescia\n",
      "50 terni\n",
      "51 vercelli\n",
      "52 lucca\n",
      "53 catanzaro\n",
      "54 pistoia\n",
      "55 vicenza\n",
      "56 novara\n",
      "57 perugia\n",
      "58 bari\n",
      "59 pavia\n",
      "60 chieti\n",
      "61 salerno\n",
      "62 verona\n",
      "63 fermo\n",
      "64 sondrio\n",
      "65 matera\n",
      "66 viterbo\n",
      "67 brindisi\n",
      "68 reggio_nell_emilia\n",
      "69 mantova\n",
      "70 pisa\n",
      "71 venezia\n",
      "72 enna\n",
      "73 oristano\n",
      "74 livorno\n",
      "75 la_spezia\n",
      "76 ferrara\n",
      "77 piacenza\n",
      "78 varese\n",
      "79 pordenone\n",
      "80 udine\n",
      "81 prato\n",
      "82 palermo\n",
      "83 sassari\n",
      "84 taranto\n",
      "85 arezzo\n",
      "86 bat\n",
      "87 treviso\n",
      "88 alessandria\n",
      "89 modena\n",
      "90 ascoli_piceno\n",
      "91 avellino\n",
      "92 verbania\n",
      "93 ragusa\n",
      "94 massa\n",
      "95 rovigo\n",
      "96 l_aquila\n",
      "97 gorizia\n",
      "98 bologna\n",
      "99 ancona\n",
      "100 trieste\n",
      "101 ravenna\n",
      "102 imperia\n",
      "103 messina\n",
      "104 grosseto\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for prov in provincelist:\n",
    "    cases = datapaper[datapaper[\"prov\"] == prov][\"Cases\"].values[0]\n",
    "    popolazione = datapaper[datapaper[\"prov\"] == prov][\"Population\"].values[0]\n",
    "    pop2 = agefeatures[agefeatures[\"prov\"] == prov][\"Population2020\"].values[0]\n",
    "    diff = 100.0*(math.fabs(popolazione-pop2)/(popolazione))\n",
    "\n",
    "    # check Exceedances/StationsNum\n",
    "    Exceedances = datapaper[datapaper[\"prov\"] == prov][\"Exceedances\"].values[0] \n",
    "    StationsNum = datapaper[datapaper[\"prov\"] == prov][\"StationsNum\"].values[0] \n",
    "\n",
    "    #if diff < 5.0 :\n",
    "    #if cases > 0.0 and diff < 5.0 and StationsNum > 0:\n",
    "    if cases > 0.0 and diff < 5.0:\n",
    "        counter += 1\n",
    "        print(counter, prov)\n",
    "\n",
    "ylogpropcasi = []\n",
    "features_dict = {}\n",
    "\n",
    "for fn in (\"population\", \"density\", \"commutersdensity\", \"depriv\", \\\n",
    "    \"lat\", \"Ratio0200ver65\",\"exoverstation\"):\n",
    "    features_dict[fn] = np.zeros(counter, dtype=\"float64\")\n",
    "\n",
    "for fn in pollutantsnames.split(\",\"):\n",
    "    features_dict[fn] = np.zeros(counter, dtype=\"float64\")\n",
    "\n",
    "i = 0 \n",
    "for idx, prov in enumerate(provincelist):\n",
    "\n",
    "    cases = datapaper[datapaper[\"prov\"] == prov][\"Cases\"].values[0]\n",
    "    popolazione = datapaper[datapaper[\"prov\"] == prov][\"Population\"].values[0]\n",
    "    pop2 = agefeatures[agefeatures[\"prov\"] == prov][\"Population2020\"].values[0]\n",
    "\n",
    "    diff = 100.0*(math.fabs(popolazione-pop2)/(popolazione))\n",
    "\n",
    "    # check Exceedances/StationsNum\n",
    "    Exceedances = datapaper[datapaper[\"prov\"] == prov][\"Exceedances\"].values[0] \n",
    "    StationsNum = datapaper[datapaper[\"prov\"] == prov][\"StationsNum\"].values[0] \n",
    "    \n",
    "    ycasi = cases/popolazione\n",
    "\n",
    "    #if diff < 5.0:\n",
    "    #if cases > 0.0 and diff < 5.0 and StationsNum > 0:\n",
    "    if cases > 0.0 and diff < 5.0:\n",
    "        if ycasi == 0.0:\n",
    "           ylogpropcasi.append(-13.0) \n",
    "        else:\n",
    "           ylogpropcasi.append(math.log(ycasi)) # atural logarithm of different numbers\n",
    "\n",
    "        selected = copernico[copernico[\"prov\"] == prov]\n",
    "\n",
    "        features_dict[\"population\"][i] = popolazione\n",
    "        features_dict[\"density\"][i] = \\\n",
    "                    datapaper[datapaper[\"prov\"] == prov][\"Density\"].values[0]    \n",
    "        features_dict[\"commutersdensity\"][i] = \\\n",
    "                    datapaper[datapaper[\"prov\"] == prov][\"CommutersDensity\"].values[0]       \n",
    "        features_dict[\"lat\"][i] = \\\n",
    "                    datapaper[datapaper[\"prov\"] == prov][\"Lat\"].values[0]       \n",
    "        features_dict[\"depriv\"][i] = \\\n",
    "                    deprividx[deprividx[\"prov\"] == prov][\"ID_2011\"].values[0]\n",
    "        #print(idx, prov, agefeatures[agefeatures[\"prov\"] == prov])\n",
    "        features_dict[\"Ratio0200ver65\"][i] = \\\n",
    "                    agefeatures[agefeatures[\"prov\"] == prov][\"Ratio0200ver65\"].values[0]\n",
    "\n",
    "        for fn in pollutantsnames.split(\",\"):\n",
    "            val = selected[fn].values[0]\n",
    "            features_dict[fn][i] = val \n",
    "\n",
    "        #features_dict[\"exoverstation\"][i] = Exceedances/StationsNum\n",
    "\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add random feature\n",
    "import random\n",
    "\n",
    "featurestobeused += \",randomfeature\"\n",
    "features_dict[\"randomfeature\"] = np.zeros(len(ylogpropcasi), dtype=\"float64\")\n",
    "\n",
    "random.seed(1)\n",
    "for i in range(len(ylogpropcasi)):\n",
    "    features_dict[\"randomfeature\"][i] = random.random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exoverstation  will be removed \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sum_wo3_ex_q75_period1_2020  will be removed \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sum_wco_ex_q75_period1_2020  will be removed \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "lat\n",
      "    sum_wno2_ex_q75_period1_2020\n",
      "avg_wco_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "    avg_wno_period1_2020\n",
      "    avg_wo3_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wnh3_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wnmvoc_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "    avg_wno_period1_2020\n",
      "    avg_wo3_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wno2_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno_period1_2020\n",
      "    avg_wo3_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wno_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "avg_wo3_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wpm10_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wnh3_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "    avg_wo3_period1_2020\n",
      "    avg_wpm2p5_period1_2020\n",
      "avg_wpm2p5_period1_2020\n",
      "    avg_wco_period1_2020\n",
      "    avg_wnh3_period1_2020\n",
      "    avg_wnmvoc_period1_2020\n",
      "    avg_wno2_period1_2020\n",
      "    avg_wo3_period1_2020\n",
      "    avg_wpm10_period1_2020\n",
      "sum_wnh3_ex_q75_period1_2020\n",
      "    sum_wnmvoc_ex_q75_period1_2020\n",
      "    sum_wpm10_ex_q75_period1_2020\n",
      "    sum_wpm2p5_ex_q75_period1_2020\n",
      "sum_wnmvoc_ex_q75_period1_2020\n",
      "    sum_wnh3_ex_q75_period1_2020\n",
      "    sum_wno2_ex_q75_period1_2020\n",
      "    sum_wpm10_ex_q75_period1_2020\n",
      "    sum_wpm2p5_ex_q75_period1_2020\n",
      "sum_wno2_ex_q75_period1_2020\n",
      "    lat\n",
      "    sum_wnmvoc_ex_q75_period1_2020\n",
      "    sum_wpm10_ex_q75_period1_2020\n",
      "    sum_wpm2p5_ex_q75_period1_2020\n",
      "sum_wpm10_ex_q75_period1_2020\n",
      "    sum_wnh3_ex_q75_period1_2020\n",
      "    sum_wnmvoc_ex_q75_period1_2020\n",
      "    sum_wno2_ex_q75_period1_2020\n",
      "    sum_wpm2p5_ex_q75_period1_2020\n",
      "sum_wpm2p5_ex_q75_period1_2020\n",
      "    sum_wnh3_ex_q75_period1_2020\n",
      "    sum_wnmvoc_ex_q75_period1_2020\n",
      "    sum_wno2_ex_q75_period1_2020\n",
      "    sum_wpm10_ex_q75_period1_2020\n",
      "Using:                        density\n",
      "Using:               commutersdensity\n",
      "Using:                         depriv\n",
      "Using:                            lat\n",
      "Using:                 Ratio0200ver65\n",
      "Using:         avg_wpm10_period1_2020\n",
      "Using:           avg_wno_period1_2020\n",
      "Using:         avg_wpans_period1_2020\n",
      "Using:          avg_wso2_period1_2020\n",
      "Using:                  randomfeature\n",
      "\n",
      "Highly correlated removing        avg_wpm2p5_period1_2020\n",
      "      avg_wco_period1_2020\n",
      "      avg_wnh3_period1_2020\n",
      "      avg_wnmvoc_period1_2020\n",
      "      avg_wno2_period1_2020\n",
      "      avg_wo3_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "Highly correlated removing           avg_wco_period1_2020\n",
      "      avg_wnmvoc_period1_2020\n",
      "      avg_wno2_period1_2020\n",
      "      avg_wno_period1_2020\n",
      "      avg_wo3_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "      avg_wpm2p5_period1_2020\n",
      "Highly correlated removing          avg_wnh3_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "      avg_wpm2p5_period1_2020\n",
      "Highly correlated removing        avg_wnmvoc_period1_2020\n",
      "      avg_wco_period1_2020\n",
      "      avg_wno2_period1_2020\n",
      "      avg_wno_period1_2020\n",
      "      avg_wo3_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "      avg_wpm2p5_period1_2020\n",
      "Highly correlated removing          avg_wno2_period1_2020\n",
      "      avg_wco_period1_2020\n",
      "      avg_wnmvoc_period1_2020\n",
      "      avg_wno_period1_2020\n",
      "      avg_wo3_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "      avg_wpm2p5_period1_2020\n",
      "Highly correlated removing           avg_wo3_period1_2020\n",
      "      avg_wco_period1_2020\n",
      "      avg_wnmvoc_period1_2020\n",
      "      avg_wno2_period1_2020\n",
      "      avg_wpm10_period1_2020\n",
      "      avg_wpm2p5_period1_2020\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# nomalize values\n",
    "new_features_dict = {}\n",
    "for fn in features_dict:\n",
    "    #print(fn)\n",
    "    abs_max = np.amax(np.abs(features_dict[fn]))\n",
    "    if abs_max == 0.0:\n",
    "        print (fn, \" will be removed \")\n",
    "        print (features_dict[fn])\n",
    "    else:\n",
    "        new_features_dict[fn] = features_dict[fn] * (1.0 / abs_max)\n",
    "\n",
    "features_dict = new_features_dict\n",
    "\n",
    "highcorrelated = {}\n",
    "for i1, v1 in enumerate(features_dict):\n",
    "    highcorrelated[v1] = []\n",
    "    for i2, v2 in enumerate(features_dict):\n",
    "        #if v1 != v2 and i2 > i1:\n",
    "        if v1 != v2:\n",
    "            corr, _ = pearsonr(features_dict[v1], features_dict[v2])\n",
    "            if math.fabs(corr) > LIMIT:\n",
    "                highcorrelated[v1].append(v2)\n",
    "                #print(v1, v2, corr)\n",
    "\n",
    "    if len(highcorrelated[v1]) > 0:\n",
    "        print(v1)\n",
    "        for fntr in highcorrelated[v1]:\n",
    "            print(\"   \", fntr)\n",
    "\n",
    "removedfeatures = []\n",
    "features = []\n",
    "for fn in featurestobeused.split(\",\"):\n",
    "    if fn in features_dict:\n",
    "        canadd = True\n",
    "        for fnin in features:\n",
    "            if fn in highcorrelated[fnin]:\n",
    "                canadd = False\n",
    "                break\n",
    "\n",
    "        if canadd:\n",
    "            print(\"Using: %30s\"%fn)\n",
    "            features.append(fn)\n",
    "        else:\n",
    "            removedfeatures.append(fn)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "for fn in removedfeatures:\n",
    "    print(\"Highly correlated removing %30s\"%fn)\n",
    "    for cf  in highcorrelated[fn]:\n",
    "        print(\"     \",cf)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguire i test cercando di minimizzare l'overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(smlmodule)\n",
    "import pprint\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_diff, best_test_rmse, best_train_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_nooverfit (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_diff, best_test_rmse, best_train_rmse)\n",
    "#pprint(besthyperF)\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "        smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout,\\\n",
    "        plotname=\"rf_model_allfeatures_optoverfit\", showplot=True, optimisedparams=besthyperF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_diff, best_test_rmse, best_train_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_nooverfit (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_diff, best_test_rmse, best_train_rmse)\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_optoverfit\", showplot=True, optimisedparams=besthyperF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF = {'bootstrap': True,\n",
    " 'ccp_alpha': 0.0,\n",
    " 'criterion': 'mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'auto',\n",
    " 'max_leaf_nodes': None,\n",
    " 'max_samples': None,\n",
    " 'min_impurity_decrease': 0.0,\n",
    " 'min_impurity_split': None,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 100,\n",
    " 'min_weight_fraction_leaf': 0.0,\n",
    " 'n_estimators': 800,\n",
    " 'n_jobs': None,\n",
    " 'oob_score': False,\n",
    " 'random_state': 1,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=5, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_optoverfit\", showplot=True, optimisedparams=besthyperF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_diff, best_test_rmse, best_train_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_nooverfit (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_diff, best_test_rmse, best_train_rmse)\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_nocommut_optoverfit\", showplot=True, optimisedparams=besthyperF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguire cercando di minimizzare invece RMSE su intero set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_rmse)\n",
    "import pprint\n",
    "\n",
    "#pprint(besthyperF)\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout,\\\n",
    "    plotname=\"rf_model_allfeatures_optoallset\", showplot=True, optimisedparams=besthyperF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_rmse)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_optallset\", showplot=True, optimisedparams=besthyperF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_rmse)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_nocommut_optallset\", showplot=True, optimisedparams=besthyperF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui invece ottimizzado RMSE sul testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_diff, best_test_rmse, best_train_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_testset (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_diff, best_test_rmse, best_train_rmse)\n",
    "#pprint(besthyperF)\n",
    "\n",
    "featuresused = features\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout,\\\n",
    "    plotname=\"rf_model_allfeatures_opttestset\", showplot=True, optimisedparams=besthyperF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF, best_diff, best_test_rmse, best_train_rmse = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_testset (X, Y, inboot=[True])\n",
    "\n",
    "#print(besthyperF, best_diff, best_test_rmse, best_train_rmse)\n",
    "\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_opttestset\", showplot=True, optimisedparams=besthyperF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(smlmodule)\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack(listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "besthyperF,best_diff, best_test_rmse, best_train_rmse  = \\\n",
    "    smlmodule.rfregressors_custom_optimizer_testset (X, Y, inboot=[True])\n",
    "\n",
    "featuresused = []\n",
    "\n",
    "for f in features:\n",
    "    if f != \"lat\" and f != \"commutersdensity\":\n",
    "        featuresused.append(f)\n",
    "\n",
    "listostack = [features_dict[v] for v in featuresused]\n",
    "\n",
    "X = np.column_stack (listostack)\n",
    "Y = np.asanyarray(ylogpropcasi)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "trainavgrmse, testavgrmse, fullsetrmse, featimport, featimport2,  \\\n",
    "        featimport3, featimport4, featimport5, featimport6 = \\\n",
    "            smlmodule.rfregressors (X, Y , featuresused, N=50, pout=sys.stdout, \\\n",
    "    plotname=\"rf_model_nolat_nocommut_opttestset\", showplot=True, optimisedparams=besthyperF)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
